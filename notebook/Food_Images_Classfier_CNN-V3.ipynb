{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import HDF5Matrix \n",
    "import h5py\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#f= h5py.File('Train/food_c101_n1000_r384x384x3.h5','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xtrain=HDF5Matrix('Train/food_c101_n10099_r32x32x1.h5','images')\n",
    "ytrain=HDF5Matrix('Train/food_c101_n10099_r32x32x1.h5','category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print ('xtrain shape is: ',xtrain.shape,', ytrain shape is: ',ytrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xtrain=np.asarray(xtrain,dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ytrain=np.asarray(ytrain,dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xtest=HDF5Matrix('Test/food_test_c101_n1000_r32x32x1.h5','images')\n",
    "ytest=HDF5Matrix('Test/food_test_c101_n1000_r32x32x1.h5','category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print ('xtest shape is: ',xtest.shape,', ytest shape is: ',ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtest=np.asarray(xtest,dtype='float32')\n",
    "ytest=np.asarray(ytest,dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "category_type=HDF5Matrix('Test/food_test_c101_n1000_r32x32x1.h5','category_names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scale the data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xtrain_scaled=xtrain/255\n",
    "xtest_scaled=xtest/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_size=xtrain.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_channel=1\n",
    "n_width=32\n",
    "n_heght=32\n",
    "n_output=101\n",
    "BATCH_SIZE=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_variable(channel,width,height,output):\n",
    "    X=tf.placeholder(tf.float32,shape=[None,height,width,channel],name='Input')\n",
    "    Y=tf.placeholder(tf.float32,shape=[None,output],name='output')\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def initialize_variable():\n",
    "    W1=tf.get_variable(\"W1\",[4,4,1,32],tf.float32,tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    W2=tf.get_variable(\"W2\",[2,2,32,32],tf.float32,tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    W3=tf.get_variable(\"W3\",[4,4,32,64],tf.float32,tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    W4=tf.get_variable(\"W4\",[2,2,64,64],tf.float32,tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    return {'W1':W1,'W2':W2,'W3':W3,'W4':W4}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Images Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def image_transformation(train):\n",
    "    image=tf.map_fn(lambda img:tf.image.random_flip_left_right(img),train)\n",
    "    image1=tf.map_fn(lambda img:tf.image.random_contrast(image,lower=0.2,upper=0.8),image)\n",
    "    image2=tf.map_fn(lambda img:tf.image.random_brightness(image,max_delta=63),image1)\n",
    "    #image3=tf.map_fn(lambda img:tf.image.per_image_standardization(image),image2)\n",
    "    #image.set_shape([32,32,1])\n",
    "    \n",
    "    return image2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_layer(X,W_filter):\n",
    "   \n",
    "    \n",
    "    #X,Y=create_variable(channel,width,height,output)\n",
    "    mean_0,variance_0=tf.nn.moments(X,axes=[0,1,2],keep_dims=True)\n",
    "    norm_X_1=tf.nn.batch_normalization(X,mean_0,variance_0,offset=0,scale=0,variance_epsilon=1e-6)\n",
    "    \n",
    "    conv1=tf.nn.conv2d(norm_X_1,W_filter['W1'],strides=[1,1,1,1],padding='SAME')\n",
    "    max_pool1=tf.nn.max_pool(tf.nn.relu(conv1),ksize=[1,2,2,1],strides=[1,1,1,1],padding='SAME')\n",
    "    dropout_1=tf.contrib.layers.dropout(max_pool1,keep_prob=0.2,is_training=True)\n",
    "    \n",
    "    norm1=tf.nn.lrn(dropout_1,depth_radius=4,bias=1.0,alpha=0.001/9.0,beta=0.4,name='norm1')\n",
    "    \n",
    "    \n",
    "    \n",
    "    conv2=tf.nn.conv2d(norm1,W_filter['W2'],strides=[1,2,2,1],padding='SAME')\n",
    "    max_pool2=tf.nn.max_pool(tf.nn.relu(conv2),ksize=[1,4,4,1],strides=[1,2,2,1],padding='SAME')\n",
    "    dropout_2=tf.contrib.layers.dropout(max_pool2,keep_prob=0.4,is_training=True)\n",
    "    \n",
    "    conv3=tf.nn.conv2d(dropout_2,W_filter['W3'],strides=[1,1,1,1],padding='SAME')\n",
    "    max_pool3=tf.nn.max_pool(tf.nn.relu(conv3),ksize=[1,2,2,1],strides=[1,1,1,1],padding='SAME')\n",
    "    dropout_3=tf.contrib.layers.dropout(max_pool3,keep_prob=0.2,is_training=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    mean_3,variance_3=tf.nn.moments(dropout_3,axes=[0,1,2],keep_dims=True)\n",
    "    dropout_3_norm=tf.nn.batch_normalization(dropout_3,mean_3,variance_3,offset=0,scale=0,variance_epsilon=1e-6)\n",
    "    \n",
    "    conv4=tf.nn.conv2d(dropout_3_norm,W_filter['W4'],strides=[1,2,2,1],padding='SAME')\n",
    "    max_pool3=tf.nn.max_pool(tf.nn.relu(conv4),ksize=[1,4,4,1],strides=[1,2,2,1],padding='SAME')\n",
    "    dropout_4=tf.contrib.layers.dropout(max_pool3,keep_prob=0.4,is_training=True)\n",
    "    \n",
    "    norm2=tf.nn.lrn(dropout_4,depth_radius=4,bias=1.0,alpha=0.001/9.0,beta=0.4,name='norm2')\n",
    "    \n",
    "    flatten=tf.contrib.layers.flatten(norm2)\n",
    "    out1=tf.contrib.layers.fully_connected(flatten,5000)\n",
    "    dropout_out1=tf.contrib.layers.dropout(out1,keep_prob=0.2,is_training=True)\n",
    "    \n",
    "    out=tf.contrib.layers.fully_connected(out1,n_output,activation_fn=None)\n",
    "    #out=tf.nn.softmax(out1)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(out,Y):\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=out,labels=Y))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def model():\n",
    "    learning_rate=0.0001\n",
    "    tf.reset_default_graph()\n",
    "    #tf.get_default_graph()\n",
    "    \n",
    "    x,y=create_variable(n_channel,n_width,n_heght,n_output)\n",
    "    W_dict=initialize_variable()\n",
    "    out = create_layer(x,W_dict)\n",
    "    loss=compute_cost(out,y)\n",
    "    global_step=tf.Variable(0)\n",
    "    learning_rate_1=tf.train.exponential_decay(0.5,global_step,5000,0.96,staircase=True)\n",
    "    optimizer=tf.train.GradientDescentOptimizer(learning_rate=learning_rate_1).minimize(loss,global_step=global_step)\n",
    "    #accuracy\n",
    "    \n",
    "    correct_pred = tf.equal(tf.argmax(out,axis=1),tf.argmax(y,axis=1))\n",
    "    accuracy=tf.reduce_mean(tf.cast(correct_pred,tf.float32))\n",
    "    init_op=tf.group(tf.global_variables_initializer(),tf.local_variables_initializer())\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "        \n",
    "        for _ in range(20):\n",
    "        \n",
    "            for i in range(0,image_size,BATCH_SIZE):\n",
    "                pos = (i % int(image_size / BATCH_SIZE)) * BATCH_SIZE\n",
    "                #indices=np.random.choice(xtrain_scaled.shape[0],128)\n",
    "                xtrain_batch,ytrain_batch=xtrain[pos:pos+BATCH_SIZE],ytrain[pos:pos+BATCH_SIZE]\n",
    "                #xtrain_batch=image_transformation(xtrain_batch)\n",
    "           \n",
    "                opt,cost=sess.run([optimizer,loss],feed_dict={x:xtrain_batch,y:ytrain_batch})\n",
    "        predictions=out.eval(feed_dict={x:xtest})    \n",
    "        print (\"Test Accuracy \",accuracy.eval(feed_dict={x:xtest,y:ytest}))\n",
    "        print (\"Train Accuracy \",accuracy.eval(feed_dict={x:xtrain,y:ytrain}))\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred=model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.argmax(pred,axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.argmax(ytest,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct=[item for item in np.equal(np.argmax(pred,axis=1),np.argmax(ytest,axis=1)) if item == True ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ytest[94]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "category_type[94]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ytrain[94]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
